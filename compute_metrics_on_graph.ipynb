{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "import graph_tool.all as gt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from typing import Generic, TypeVar, cast\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "import random, json\n",
    "import networkx as nx\n",
    "import datetime, os\n",
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph\n",
      "graph loaded\n",
      "MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    }
   ],
   "source": [
    "# loading the networkx graph\n",
    "nxG = None\n",
    "print(\"loading graph\")\n",
    "with open('graph.pkl', 'rb') as f:\n",
    "    nxG = dill.load(f)\n",
    "print(\"graph loaded\")\n",
    "print(nxG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a method that keeps nodes from a list\n",
    "# and it should also remove edges between nodes of a certain kind, if provided\n",
    "def filter_graph(G: nx.Graph, node_labels_to_keep: list=[], edges_to_keep: List[Tuple]=[]) -> nx.Graph:\n",
    "    '''\n",
    "    takes in a graph G, a list of node labels to keep i.e. [\"User\", \"Device\"]\n",
    "    and a list of tuples (order doesn't matter) saying between which kinds of nodes\n",
    "    we want to keep edges, i.e. [(\"User\", \"Device\")]\n",
    "    '''\n",
    "    print(\"graph before filtering: \", G)\n",
    "\n",
    "    # first find all nodes to remove\n",
    "    if len(node_labels_to_keep) > 0:\n",
    "        nodes_to_remove = []\n",
    "        attrs = nx.get_node_attributes(G, 'labels') # this makes it faster\n",
    "        for n in tqdm(G.nodes()):\n",
    "            # print(list(nx.get_node_attributes(G, 'labels')[n])[0])\n",
    "            if list(attrs[n])[0] not in node_labels_to_keep:\n",
    "                nodes_to_remove.append(n)\n",
    "        \n",
    "        # then remove them\n",
    "        Gnew = copy.deepcopy(G)\n",
    "        Gnew.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "    # then find all edges to remove\n",
    "    if len(edges_to_keep) > 0:\n",
    "        edges_to_delete = []\n",
    "        attrsnew = nx.get_node_attributes(Gnew, 'labels')\n",
    "        for u, v, attr in Gnew.edges(data=True):\n",
    "            ntu = list(attrsnew[u])[0]\n",
    "            ntv = list(attrsnew[v])[0]\n",
    "            keep = False\n",
    "            for edge_tuple in edges_to_keep:\n",
    "                if str((ntu, ntv)) == edge_tuple or str((ntv, ntu)) == edge_tuple:\n",
    "                    keep = True\n",
    "            if not keep:\n",
    "                edges_to_delete.append((u, v))\n",
    "\n",
    "        # then remove them\n",
    "        Gnew.remove_edges_from(edges_to_delete)\n",
    "\n",
    "    print(\"graph after filtering:\", Gnew)\n",
    "\n",
    "    return Gnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining graph_tool methods\n",
    "\n",
    "def convert_networkx_to_graphtool(nx_graph):\n",
    "    # print(\"part_1\")\n",
    "    gt_graph = gt.Graph(directed=nx_graph.is_directed())\n",
    "    # print(\"part_2\")\n",
    "    gt_graph.add_vertex(len(nx_graph))\n",
    "\n",
    "    # Create a mapping between original labels and new integer labels\n",
    "    # print(\"part_3\")\n",
    "    label_to_index = {label: index for index, label in enumerate(nx_graph.nodes)}\n",
    "    index_to_label = {index: label for index, label in enumerate(nx_graph.nodes)}\n",
    "\n",
    "    print(\"adding edges\")\n",
    "    for edge in tqdm(nx_graph.edges):\n",
    "        gt_graph.add_edge(label_to_index[edge[0]], label_to_index[edge[1]])\n",
    "\n",
    "    return gt_graph, label_to_index, index_to_label\n",
    "\n",
    "def compute_betweenness(g):\n",
    "    vb, eb = gt.betweenness(g)\n",
    "    betweenness = {}\n",
    "    for v in g.vertices():\n",
    "        betweenness[int(v)] = vb[v]\n",
    "    return betweenness\n",
    "\n",
    "def compute_eigenvector_centrality(g):\n",
    "    _, ev = gt.eigenvector(g)\n",
    "    eigenvector_centrality = {}\n",
    "    for v in g.vertices():\n",
    "        eigenvector_centrality[int(v)] = ev[v]\n",
    "    return eigenvector_centrality\n",
    "\n",
    "def compute_closeness(g):\n",
    "    closeness = gt.closeness(g)\n",
    "    closeness_dict = {}\n",
    "    for v in g.vertices():\n",
    "        closeness_dict[int(v)] = closeness[v]\n",
    "    return closeness_dict\n",
    "\n",
    "def compute_degree(g):\n",
    "    degree_dict = {}\n",
    "    for v in g.vertices():\n",
    "        degree_dict[int(v)] = v.out_degree()\n",
    "    return degree_dict\n",
    "\n",
    "def compute_SIS_infection_prob(g, beta, mu):\n",
    "    N = g.num_vertices()\n",
    "    infection_prob = [0] * N\n",
    "    for i in range(N):\n",
    "        infection_prob[i] = 1 - (1 - beta) ** (mu * g.vertex(i).out_degree())\n",
    "    return infection_prob\n",
    "\n",
    "def compute_page_rank(g, damping=0.85, epsilon=1e-6):\n",
    "    pr = gt.pagerank(g, damping=damping, epsilon=epsilon)\n",
    "    page_rank = {}\n",
    "    for v in g.vertices():\n",
    "        page_rank[int(v)] = pr[v]\n",
    "    return page_rank\n",
    "\n",
    "    \n",
    "# def SI_infection_simulation(g, n, t, label_probabilities=None):\n",
    "#     N = g.num_vertices()\n",
    "#     infected_count = [0] * t\n",
    "\n",
    "#     def infect_node(start_node, timesteps):\n",
    "#         infected = defaultdict(bool)\n",
    "#         infected[start_node] = True\n",
    "#         new_infected = [start_node]\n",
    "#         count = 1\n",
    "\n",
    "#         for t in range(timesteps):\n",
    "#             if count >= n:\n",
    "#                 break\n",
    "#             next_infected = []\n",
    "\n",
    "#             for node in new_infected:\n",
    "#                 neighbors = [v for v in node.out_neighbors()]\n",
    "#                 for neighbor in neighbors:\n",
    "#                     if not infected[int(neighbor)]:\n",
    "#                         if label_probabilities:\n",
    "#                             infector_label = g.vp.label[node]\n",
    "#                             infected_label = g.vp.label[neighbor]\n",
    "#                             infection_prob = label_probabilities[(infector_label, infected_label)]\n",
    "#                         else:\n",
    "#                             infection_prob = 1\n",
    "\n",
    "#                         if random() < infection_prob:\n",
    "#                             infected[int(neighbor)] = True\n",
    "#                             next_infected.append(neighbor)\n",
    "#                             count += 1\n",
    "#                             if count >= n:\n",
    "#                                 break\n",
    "\n",
    "#             new_infected = next_infected\n",
    "#             infected_count[t] += count\n",
    "\n",
    "#     for node in tqdm(g.vertices()):\n",
    "#         infect_node(node, t)\n",
    "\n",
    "#     return [count / N for count in infected_count]\n",
    "\n",
    "# def SI_infection_simulation_timestamps(g, n, t, nx_graph, label_to_index, index_to_label, label_probabilities=None):\n",
    "#     nx_nodeattrs = nx.get_node_attributes(nx_graph, 'labels')\n",
    "#     nx_edgeattrs = nx.get_edge_attributes(nx_graph, 'type')\n",
    "#     def get_timestamp_from_edge(n1, n2):\n",
    "        \n",
    "\n",
    "#     N = g.num_vertices()\n",
    "#     infected_count = [0] * t\n",
    "\n",
    "#     def infect_node(start_node, timesteps):\n",
    "#         infected = defaultdict(bool)\n",
    "#         infected[start_node] = True\n",
    "#         new_infected = [start_node]\n",
    "#         count = 1\n",
    "\n",
    "#         for current_t in range(timesteps):\n",
    "#             if count >= n:\n",
    "#                 break\n",
    "#             next_infected = []\n",
    "\n",
    "#             for node in new_infected:\n",
    "#                 neighbors = [(v, g.ep.timestamps[e]) for e, v in zip(node.out_edges(), node.out_neighbors())]\n",
    "\n",
    "#                 for neighbor, timestamps in neighbors:\n",
    "#                     if not infected[int(neighbor)] and current_t in timestamps:\n",
    "#                         if label_probabilities:\n",
    "#                             infector_label = g.vp.label[node]\n",
    "#                             infected_label = g.vp.label[neighbor]\n",
    "#                             infection_prob = label_probabilities[(infector_label, infected_label)]\n",
    "#                         else:\n",
    "#                             infection_prob = 1\n",
    "\n",
    "#                         if random.random() < infection_prob:\n",
    "#                             infected[int(neighbor)] = True\n",
    "#                             next_infected.append(neighbor)\n",
    "#                             count += 1\n",
    "#                             if count >= n:\n",
    "#                                 break\n",
    "\n",
    "#             new_infected = next_infected\n",
    "#             infected_count[current_t] += count\n",
    "\n",
    "#     for node in g.vertices():\n",
    "#         infect_node(node, t)\n",
    "\n",
    "#     return [count / N for count in infected_count]\n",
    "\n",
    "def get_edge_time(edge_data):\n",
    "    if 'transactionDateTime' in edge_data:\n",
    "        return edge_data['transactionDateTime']\n",
    "    elif 'cardDate' in edge_data:\n",
    "        return datetime.datetime.combine(edge_data['cardDate'], datetime.time())\n",
    "    elif 'deviceDate' in edge_data:\n",
    "        return datetime.datetime.combine(edge_data['deviceDate'], datetime.time())\n",
    "    elif 'ipDate' in edge_data:\n",
    "        return datetime.datetime.combine(edge_data['ipDate'], datetime.time())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def _si_infection_step(args):\n",
    "    graph, node, t, label_prob = args\n",
    "    infected_nodes = set([node])\n",
    "    new_infected_nodes = set([node])\n",
    "\n",
    "    for _ in range(t):\n",
    "        next_new_infected_nodes = set()\n",
    "        for infected_node in new_infected_nodes:\n",
    "            neighbors = list(graph.neighbors(infected_node))\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in infected_nodes and random.random() <= label_prob[list(graph.nodes[infected_node]['labels'])[0]][list(graph.nodes[neighbor]['labels'])[0]]:\n",
    "                    next_new_infected_nodes.add(neighbor)\n",
    "                    infected_nodes.add(neighbor)\n",
    "        new_infected_nodes = next_new_infected_nodes\n",
    "    return len(infected_nodes)\n",
    "\n",
    "def si_infection_simulation(graph, n=100, t=100, label_prob=None):\n",
    "    if label_prob is None:\n",
    "        label_prob = {\n",
    "            # label1: {label2: 1.0 for label2 in graph.graph['Label']} for label1 in graph.graph['Label']\n",
    "            \"User\": {\n",
    "                \"User\": 1,\n",
    "                \"Device\": 1,\n",
    "                \"Card\": 1,\n",
    "                \"IP\": 1,\n",
    "            },\n",
    "            \"Device\": {\n",
    "                \"User\": 1,\n",
    "                \"Device\": 1,\n",
    "                \"Card\": 1,\n",
    "                \"IP\": 1,\n",
    "            },\n",
    "            \"Card\": {\n",
    "                \"User\": 1,\n",
    "                \"Device\": 1,\n",
    "                \"Card\": 1,\n",
    "                \"IP\": 1,\n",
    "            },\n",
    "            \"IP\": {\n",
    "                \"User\": 1,\n",
    "                \"Device\": 1,\n",
    "                \"Card\": 1,\n",
    "                \"IP\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    nodes = list(graph.nodes)\n",
    "    args = [(graph, node, t, label_prob) for node in nodes]\n",
    "\n",
    "    with Pool() as pool:\n",
    "        infection_counts = list(tqdm(pool.imap_unordered(_si_infection_step, args), total=len(args), desc=\"Infection simulation\"))\n",
    "\n",
    "    return infection_counts\n",
    "\n",
    "# Example usage:\n",
    "# G = ... # Load the graph\n",
    "# n = 5\n",
    "# t = 10\n",
    "# label_prob = { ... } # Optional custom label probabilities\n",
    "# infection_counts = si_infection_simulation(G, n, t, label_prob)\n",
    "# print(infection_counts)\n",
    "\n",
    "# Example usage:\n",
    "# label_probabilities = {(\"A\", \"A\"): 0.9, (\"A\", \"B\"): 0.5, (\"B\", \"A\"): 0.5, (\"B\", \"B\"): 0.9}\n",
    "# infected_counts = SI_infection_simulation_timestamps(g, 10, 100, label_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing metrics\n",
    "def compute_metrics(G: gt.Graph, index_to_label: dict):\n",
    "\n",
    "    g = G\n",
    "\n",
    "    beta = 0.5\n",
    "    mu = 0.3\n",
    "\n",
    "    print(\"computing closeness\")\n",
    "    closeness = compute_closeness(g)\n",
    "\n",
    "    print(\"computing degree\")\n",
    "    degree = compute_degree(g)\n",
    "\n",
    "    print(\"computing betweenness\")\n",
    "    betweenness = compute_betweenness(g)\n",
    "\n",
    "    print(\"computing eigenvector_centrality\")\n",
    "    eigenvector_centrality = compute_eigenvector_centrality(g)\n",
    "\n",
    "    print(\"computing SIS_infection_prob\")\n",
    "    SIS_infection_prob = compute_SIS_infection_prob(g, beta, mu)\n",
    "\n",
    "    print(\"computing pagerank\")\n",
    "    page_rank = compute_page_rank(g)\n",
    "\n",
    "    print(\"computation done\")\n",
    "\n",
    "    # print(f\"Node : Betweenness : Eigenvector Centrality : Closeness : Degree : SIS Infection Probability : Page Rank\")\n",
    "    # for node in list(g.vertices())[:10]:\n",
    "    #     print(f\"{int(node)} : {betweenness[int(node)]} : {eigenvector_centrality[int(node)]} : {closeness[int(node)]} : {degree[int(node)]} : {SIS_infection_prob[int(node)]} : {page_rank[int(node)]}\")\n",
    "    #     # print(f\"Node {int(node)}: Betweenness: {betweenness[int(node)]}, Eigenvector Centrality: {eigenvector_centrality[int(node)]}, Closeness: {closeness[int(node)]}, Degree: {degree[int(node)]}, SIS Infection Probability: {SIS_infection_prob[int(node)]}\")\n",
    "    \n",
    "    data_dict = {}\n",
    "    for node in g.vertices():\n",
    "        data_dict[index_to_label[int(node)]] = dict(\n",
    "            closeness=closeness[int(node)],\n",
    "            degree=degree[int(node)],\n",
    "            betweenness=betweenness[int(node)],\n",
    "            eigenvector_centrality=eigenvector_centrality[int(node)],\n",
    "            sis_infection_prob=SIS_infection_prob[int(node)],\n",
    "            page_rank=page_rank[int(node)],\n",
    "        )\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphs(nx_graph, gt_graph, graph_name, label_to_index, index_to_label):\n",
    "    if not os.path.exists('graphs'):\n",
    "        os.makedirs('graphs')\n",
    "\n",
    "    nx_file = os.path.join('graphs', f'{graph_name}_networkx.dill')\n",
    "    gt_file = os.path.join('graphs', f'{graph_name}_graphtool.dill')\n",
    "\n",
    "    if not os.path.exists(nx_file):\n",
    "        with open(nx_file, 'wb') as f:\n",
    "            dill.dump(nx_graph, f)\n",
    "\n",
    "    if not os.path.exists(gt_file):\n",
    "        with open(gt_file, 'wb') as f:\n",
    "            dill.dump((gt_graph, label_to_index, index_to_label), f)\n",
    "\n",
    "def load_graphs(graph_name):\n",
    "    nx_file = os.path.join('graphs', f'{graph_name}_networkx.dill')\n",
    "    gt_file = os.path.join('graphs', f'{graph_name}_graphtool.dill')\n",
    "\n",
    "    if os.path.exists(nx_file) and os.path.exists(gt_file):\n",
    "        with open(nx_file, 'rb') as f:\n",
    "            nx_graph = dill.load(f)\n",
    "\n",
    "        with open(gt_file, 'rb') as f:\n",
    "            gt_graph, label_to_index, index_to_label = dill.load(f)\n",
    "\n",
    "        return nx_graph, gt_graph, label_to_index, index_to_label\n",
    "    else:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "computing graph  only_users\n",
      "graph before filtering:  MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 789856/789856 [00:00<00:00, 2376665.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# the experiment then goes like this:\n",
    "# 1. instantiate the graphs you're interested in\n",
    "# 2. run analysis on each, only keep the data_dict\n",
    "# 3. put the dicts together in a dict with simple names\n",
    "\n",
    "graphs_we_want = {\n",
    "    \"only_users\": dict(\n",
    "        node_labels_to_keep=['User'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    # \"users_and_cards\": dict(\n",
    "    #     node_labels_to_keep=['User', 'Card'],\n",
    "    #     edges_to_keep=[],\n",
    "    # ),\n",
    "    # \"users_and_devices\": dict(\n",
    "    #     node_labels_to_keep=['User', 'Device'],\n",
    "    #     edges_to_keep=[],\n",
    "    # ),\n",
    "    # \"users_and_ips\": dict(\n",
    "    #     node_labels_to_keep=['User', 'IP'],\n",
    "    #     edges_to_keep=[],\n",
    "    # ),\n",
    "    # \"users_and_cards_without_user_to_user_edges\": dict(\n",
    "    #     node_labels_to_keep=['User', 'Card'],\n",
    "    #     edges_to_keep=[('User', 'Card')],\n",
    "    # ),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for graph_name, graph_info in graphs_we_want.items():\n",
    "    print(\"\\n\\ncomputing graph \", graph_name)\n",
    "    # 1. instantiate the graphs you're interested in\n",
    "    nx_graph, gt_graph, label_to_index, index_to_label = load_graphs(graph_name)\n",
    "    if nx_graph is None or gt_graph is None:\n",
    "        nx_graph = filter_graph(nxG, graph_info['node_labels_to_keep'], graph_info['edges_to_keep'])\n",
    "        gt_graph, label_to_index, index_to_label = convert_networkx_to_graphtool(nx_graph)\n",
    "        save_graphs(nx_graph, gt_graph, graph_name, label_to_index, index_to_label)\n",
    "\n",
    "    # 2. run analysis on each, only keep the data_dict\n",
    "    result = compute_metrics(gt_graph, index_to_label)\n",
    "    \n",
    "    # 3. put the dicts together in a dict with simple names\n",
    "    # results[graph_name] = result\n",
    "    results = si_infection_simulation(nx_graph)\n",
    "    print(results)\n",
    "# nx_graph = filter_graph(nxG, graphs_we_want[\"users_and_cards\"]['node_labels_to_keep'], graphs_we_want[\"users_and_cards\"]['edges_to_keep'])\n",
    "# gt_graph, label_to_index, index_to_label = convert_networkx_to_graphtool(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(gt_graph.vertices())[:10])\n",
    "# vertex = list(gt_graph.vertices())[154]\n",
    "# print(dir(vertex))\n",
    "# vertex object attributes\n",
    "# 'all_edges', 'all_neighbors', 'all_neighbours', 'graph_ptr', 'graph_type',\n",
    "# 'in_degree', 'in_edges', 'in_neighbors', 'in_neighbours', 'is_valid',\n",
    "# 'out_degree', 'out_edges', 'out_neighbors', 'out_neighbours'\n",
    "\n",
    "# for v in gt_graph.vertices():\n",
    "#     v['name'] = \"brian\"\n",
    "#     for prop in v.vp:\n",
    "#         print(prop)\n",
    "\n",
    "# edat = nx.get_edge_attributes(nx_graph, 'properties')\n",
    "# # for k, v in list(edat.items())[:10]:\n",
    "# #     print(k, v)\n",
    "# for u, v, keys, attrs in list(nx_graph.edges(data=True, keys=True))[:10]:\n",
    "#     print(attrs)\n",
    "#     # print(u, v)\n",
    "#     print(edat[(u, v, keys)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"compute_metrics_results.json\", 'w') as f:\n",
    "#     json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
