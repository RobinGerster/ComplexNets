{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "import graph_tool.all as gt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from typing import Generic, TypeVar, cast\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the networkx graph\n",
    "nxG = None\n",
    "print(\"loading graph\")\n",
    "with open('graph.pkl', 'rb') as f:\n",
    "    nxG = dill.load(f)\n",
    "print(\"graph loaded\")\n",
    "print(nxG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a method that keeps nodes from a list\n",
    "# and it should also remove edges between nodes of a certain kind, if provided\n",
    "def filter_graph(G: nx.Graph, node_labels_to_keep: list=[], edges_to_keep: List[Tuple]=[]) -> nx.Graph:\n",
    "    '''\n",
    "    takes in a graph G, a list of node labels to keep i.e. [\"User\", \"Device\"]\n",
    "    and a list of tuples (order doesn't matter) saying between which kinds of nodes\n",
    "    we want to keep edges, i.e. [(\"User\", \"Device\")]\n",
    "    '''\n",
    "    print(\"graph before filtering: \", G)\n",
    "\n",
    "    # first find all nodes to remove\n",
    "    if len(node_labels_to_keep) > 0:\n",
    "        nodes_to_remove = []\n",
    "        attrs = nx.get_node_attributes(G, 'labels') # this makes it faster\n",
    "        for n in tqdm(G.nodes()):\n",
    "            # print(list(nx.get_node_attributes(G, 'labels')[n])[0])\n",
    "            if list(attrs[n])[0] not in node_labels_to_keep:\n",
    "                nodes_to_remove.append(n)\n",
    "        \n",
    "        # then remove them\n",
    "        Gnew = copy.deepcopy(G)\n",
    "        Gnew.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "    # then find all edges to remove\n",
    "    if len(edges_to_keep) > 0:\n",
    "        edges_to_delete = []\n",
    "        attrsnew = nx.get_node_attributes(Gnew, 'labels')\n",
    "        for u, v, attr in Gnew.edges(data=True):\n",
    "            ntu = list(attrsnew[u])[0]\n",
    "            ntv = list(attrsnew[v])[0]\n",
    "            keep = False\n",
    "            for edge_tuple in edges_to_keep:\n",
    "                if str((ntu, ntv)) == edge_tuple or str((ntv, ntu)) == edge_tuple:\n",
    "                    keep = True\n",
    "            if not keep:\n",
    "                edges_to_delete.append((u, v))\n",
    "\n",
    "        # then remove them\n",
    "        Gnew.remove_edges_from(edges_to_delete)\n",
    "\n",
    "    print(\"graph after filtering:\", Gnew)\n",
    "\n",
    "    return Gnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining graph_tool methods\n",
    "\n",
    "def convert_networkx_to_graphtool(nx_graph):\n",
    "    print(\"part_1\")\n",
    "    gt_graph = gt.Graph(directed=nx_graph.is_directed())\n",
    "    print(\"part_2\")\n",
    "    gt_graph.add_vertex(len(nx_graph))\n",
    "\n",
    "    # Create a mapping between original labels and new integer labels\n",
    "    print(\"part_3\")\n",
    "    label_to_index = {label: index for index, label in enumerate(nx_graph.nodes)}\n",
    "    index_to_label = {index: label for index, label in enumerate(nx_graph.nodes)}\n",
    "\n",
    "    print(\"part_4\")\n",
    "    for edge in tqdm(nx_graph.edges):\n",
    "        gt_graph.add_edge(label_to_index[edge[0]], label_to_index[edge[1]])\n",
    "\n",
    "    return gt_graph, label_to_index, index_to_label\n",
    "\n",
    "def compute_betweenness(g):\n",
    "    vb, eb = gt.betweenness(g)\n",
    "    betweenness = {}\n",
    "    for v in g.vertices():\n",
    "        betweenness[int(v)] = vb[v]\n",
    "    return betweenness\n",
    "\n",
    "def compute_eigenvector_centrality(g):\n",
    "    _, ev = gt.eigenvector(g)\n",
    "    eigenvector_centrality = {}\n",
    "    for v in g.vertices():\n",
    "        eigenvector_centrality[int(v)] = ev[v]\n",
    "    return eigenvector_centrality\n",
    "\n",
    "def compute_closeness(g):\n",
    "    closeness = gt.closeness(g)\n",
    "    closeness_dict = {}\n",
    "    for v in g.vertices():\n",
    "        closeness_dict[int(v)] = closeness[v]\n",
    "    return closeness_dict\n",
    "\n",
    "def compute_degree(g):\n",
    "    degree_dict = {}\n",
    "    for v in g.vertices():\n",
    "        degree_dict[int(v)] = v.out_degree()\n",
    "    return degree_dict\n",
    "\n",
    "def compute_SIS_infection_prob(g, beta, mu):\n",
    "    N = g.num_vertices()\n",
    "    infection_prob = [0] * N\n",
    "    for i in range(N):\n",
    "        infection_prob[i] = 1 - (1 - beta) ** (mu * g.vertex(i).out_degree())\n",
    "    return infection_prob\n",
    "\n",
    "def compute_page_rank(g, damping=0.85, epsilon=1e-6):\n",
    "    pr = gt.pagerank(g, damping=damping, epsilon=epsilon)\n",
    "    page_rank = {}\n",
    "    for v in g.vertices():\n",
    "        page_rank[int(v)] = pr[v]\n",
    "    return page_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing metrics\n",
    "def compute_metrics(G: gt.Graph, index_to_label: dict):\n",
    "\n",
    "    g = G\n",
    "\n",
    "    beta = 0.5\n",
    "    mu = 0.3\n",
    "\n",
    "    print(\"computing closeness\")\n",
    "    closeness = compute_closeness(g)\n",
    "\n",
    "    print(\"computing degree\")\n",
    "    degree = compute_degree(g)\n",
    "\n",
    "    print(\"computing betweenness\")\n",
    "    betweenness = compute_betweenness(g)\n",
    "\n",
    "    print(\"computing eigenvector_centrality\")\n",
    "    eigenvector_centrality = compute_eigenvector_centrality(g)\n",
    "\n",
    "    print(\"computing SIS_infection_prob\")\n",
    "    SIS_infection_prob = compute_SIS_infection_prob(g, beta, mu)\n",
    "\n",
    "    print(\"computing pagerank\")\n",
    "    page_rank = compute_page_rank(g)\n",
    "\n",
    "    print(\"computation done\")\n",
    "\n",
    "    # print(f\"Node : Betweenness : Eigenvector Centrality : Closeness : Degree : SIS Infection Probability : Page Rank\")\n",
    "    # for node in list(g.vertices())[:10]:\n",
    "    #     print(f\"{int(node)} : {betweenness[int(node)]} : {eigenvector_centrality[int(node)]} : {closeness[int(node)]} : {degree[int(node)]} : {SIS_infection_prob[int(node)]} : {page_rank[int(node)]}\")\n",
    "    #     # print(f\"Node {int(node)}: Betweenness: {betweenness[int(node)]}, Eigenvector Centrality: {eigenvector_centrality[int(node)]}, Closeness: {closeness[int(node)]}, Degree: {degree[int(node)]}, SIS Infection Probability: {SIS_infection_prob[int(node)]}\")\n",
    "    \n",
    "    data_dict = {}\n",
    "    for node in g.vertices():\n",
    "        data_dict[index_to_label[int(node)]] = dict(\n",
    "            closeness=closeness[int(node)],\n",
    "            degree=degree[int(node)],\n",
    "            betweenness=betweenness[int(node)],\n",
    "            eigenvector_centrality=eigenvector_centrality[int(node)],\n",
    "            sis_infection_prob=SIS_infection_prob[int(node)],\n",
    "            page_rank=page_rank[int(node)],\n",
    "        )\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the experiment then goes like this:\n",
    "# 1. instantiate the graphs you're interested in\n",
    "# 2. run analysis on each, only keep the data_dict\n",
    "# 3. put the dicts together in a dict with simple names\n",
    "\n",
    "graphs_we_want = {\n",
    "    \"only_users\": dict(\n",
    "        node_labels_to_keep=['User'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_cards\": dict(\n",
    "        node_labels_to_keep=['User', 'Card'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_devices\": dict(\n",
    "        node_labels_to_keep=['User', 'Device'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_ips\": dict(\n",
    "        node_labels_to_keep=['User', 'IP'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_cards_without_user_to_user_edges\": dict(\n",
    "        node_labels_to_keep=['User', 'Card'],\n",
    "        edges_to_keep=[('User', 'Card')],\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for graph_name, graph_info in graphs_we_want.items():\n",
    "    # 1. instantiate the graphs you're interested in\n",
    "    nx_graph = filter_graph(nxG, graph_info['node_labels_to_keep'], graph_info['edges_to_keep'])\n",
    "    # 2. run analysis on each, only keep the data_dict\n",
    "    gt_graph, label_to_index, index_to_label = convert_networkx_to_graphtool(nx_graph)\n",
    "    result = compute_metrics(gt_graph, index_to_label)\n",
    "    # 3. put the dicts together in a dict with simple names\n",
    "    results[graph_name] = result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
