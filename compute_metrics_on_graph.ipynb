{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "import graph_tool.all as gt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from typing import Generic, TypeVar, cast\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph\n",
      "graph loaded\n",
      "MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    }
   ],
   "source": [
    "# loading the networkx graph\n",
    "nxG = None\n",
    "print(\"loading graph\")\n",
    "with open('graph.pkl', 'rb') as f:\n",
    "    nxG = dill.load(f)\n",
    "print(\"graph loaded\")\n",
    "print(nxG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a method that keeps nodes from a list\n",
    "# and it should also remove edges between nodes of a certain kind, if provided\n",
    "def filter_graph(G: nx.Graph, node_labels_to_keep: list=[], edges_to_keep: List[Tuple]=[]) -> nx.Graph:\n",
    "    '''\n",
    "    takes in a graph G, a list of node labels to keep i.e. [\"User\", \"Device\"]\n",
    "    and a list of tuples (order doesn't matter) saying between which kinds of nodes\n",
    "    we want to keep edges, i.e. [(\"User\", \"Device\")]\n",
    "    '''\n",
    "    print(\"graph before filtering: \", G)\n",
    "\n",
    "    # first find all nodes to remove\n",
    "    if len(node_labels_to_keep) > 0:\n",
    "        nodes_to_remove = []\n",
    "        attrs = nx.get_node_attributes(G, 'labels') # this makes it faster\n",
    "        for n in tqdm(G.nodes()):\n",
    "            # print(list(nx.get_node_attributes(G, 'labels')[n])[0])\n",
    "            if list(attrs[n])[0] not in node_labels_to_keep:\n",
    "                nodes_to_remove.append(n)\n",
    "        \n",
    "        # then remove them\n",
    "        Gnew = copy.deepcopy(G)\n",
    "        Gnew.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "    # then find all edges to remove\n",
    "    if len(edges_to_keep) > 0:\n",
    "        edges_to_delete = []\n",
    "        attrsnew = nx.get_node_attributes(Gnew, 'labels')\n",
    "        for u, v, attr in Gnew.edges(data=True):\n",
    "            ntu = list(attrsnew[u])[0]\n",
    "            ntv = list(attrsnew[v])[0]\n",
    "            keep = False\n",
    "            for edge_tuple in edges_to_keep:\n",
    "                if str((ntu, ntv)) == edge_tuple or str((ntv, ntu)) == edge_tuple:\n",
    "                    keep = True\n",
    "            if not keep:\n",
    "                edges_to_delete.append((u, v))\n",
    "\n",
    "        # then remove them\n",
    "        Gnew.remove_edges_from(edges_to_delete)\n",
    "\n",
    "    print(\"graph after filtering:\", Gnew)\n",
    "\n",
    "    return Gnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining graph_tool methods\n",
    "\n",
    "def convert_networkx_to_graphtool(nx_graph):\n",
    "    # print(\"part_1\")\n",
    "    gt_graph = gt.Graph(directed=nx_graph.is_directed())\n",
    "    # print(\"part_2\")\n",
    "    gt_graph.add_vertex(len(nx_graph))\n",
    "\n",
    "    # Create a mapping between original labels and new integer labels\n",
    "    # print(\"part_3\")\n",
    "    label_to_index = {label: index for index, label in enumerate(nx_graph.nodes)}\n",
    "    index_to_label = {index: label for index, label in enumerate(nx_graph.nodes)}\n",
    "\n",
    "    print(\"adding edges\")\n",
    "    for edge in tqdm(nx_graph.edges):\n",
    "        gt_graph.add_edge(label_to_index[edge[0]], label_to_index[edge[1]])\n",
    "\n",
    "    return gt_graph, label_to_index, index_to_label\n",
    "\n",
    "def compute_betweenness(g):\n",
    "    vb, eb = gt.betweenness(g)\n",
    "    betweenness = {}\n",
    "    for v in g.vertices():\n",
    "        betweenness[int(v)] = vb[v]\n",
    "    return betweenness\n",
    "\n",
    "def compute_eigenvector_centrality(g):\n",
    "    _, ev = gt.eigenvector(g)\n",
    "    eigenvector_centrality = {}\n",
    "    for v in g.vertices():\n",
    "        eigenvector_centrality[int(v)] = ev[v]\n",
    "    return eigenvector_centrality\n",
    "\n",
    "def compute_closeness(g):\n",
    "    closeness = gt.closeness(g)\n",
    "    closeness_dict = {}\n",
    "    for v in g.vertices():\n",
    "        closeness_dict[int(v)] = closeness[v]\n",
    "    return closeness_dict\n",
    "\n",
    "def compute_degree(g):\n",
    "    degree_dict = {}\n",
    "    for v in g.vertices():\n",
    "        degree_dict[int(v)] = v.out_degree()\n",
    "    return degree_dict\n",
    "\n",
    "def compute_SIS_infection_prob(g, beta, mu):\n",
    "    N = g.num_vertices()\n",
    "    infection_prob = [0] * N\n",
    "    for i in range(N):\n",
    "        infection_prob[i] = 1 - (1 - beta) ** (mu * g.vertex(i).out_degree())\n",
    "    return infection_prob\n",
    "\n",
    "def compute_page_rank(g, damping=0.85, epsilon=1e-6):\n",
    "    pr = gt.pagerank(g, damping=damping, epsilon=epsilon)\n",
    "    page_rank = {}\n",
    "    for v in g.vertices():\n",
    "        page_rank[int(v)] = pr[v]\n",
    "    return page_rank\n",
    "\n",
    "    \n",
    "def SI_infection_simulation(g, n, t, label_probabilities=None):\n",
    "    N = g.num_vertices()\n",
    "    infected_count = [0] * t\n",
    "\n",
    "    def infect_node(start_node, timesteps):\n",
    "        infected = defaultdict(bool)\n",
    "        infected[start_node] = True\n",
    "        new_infected = [start_node]\n",
    "        count = 1\n",
    "\n",
    "        for t in range(timesteps):\n",
    "            if count >= n:\n",
    "                break\n",
    "            next_infected = []\n",
    "\n",
    "            for node in new_infected:\n",
    "                neighbors = [v for v in node.out_neighbors()]\n",
    "                for neighbor in neighbors:\n",
    "                    if not infected[int(neighbor)]:\n",
    "                        if label_probabilities:\n",
    "                            infector_label = g.vp.label[node]\n",
    "                            infected_label = g.vp.label[neighbor]\n",
    "                            infection_prob = label_probabilities[(infector_label, infected_label)]\n",
    "                        else:\n",
    "                            infection_prob = 1\n",
    "\n",
    "                        if random() < infection_prob:\n",
    "                            infected[int(neighbor)] = True\n",
    "                            next_infected.append(neighbor)\n",
    "                            count += 1\n",
    "                            if count >= n:\n",
    "                                break\n",
    "\n",
    "            new_infected = next_infected\n",
    "            infected_count[t] += count\n",
    "\n",
    "    for node in tqdm(g.vertices()):\n",
    "        infect_node(node, t)\n",
    "\n",
    "    return [count / N for count in infected_count]\n",
    "\n",
    "def SI_infection_simulation_timestamps(g, n, t, label_probabilities=None):\n",
    "    N = g.num_vertices()\n",
    "    infected_count = [0] * t\n",
    "\n",
    "    def infect_node(start_node, timesteps):\n",
    "        infected = defaultdict(bool)\n",
    "        infected[start_node] = True\n",
    "        new_infected = [start_node]\n",
    "        count = 1\n",
    "\n",
    "        for current_t in range(timesteps):\n",
    "            if count >= n:\n",
    "                break\n",
    "            next_infected = []\n",
    "\n",
    "            for node in new_infected:\n",
    "                neighbors = [(v, g.ep.timestamps[e]) for e, v in zip(node.out_edges(), node.out_neighbors())]\n",
    "\n",
    "                for neighbor, timestamps in neighbors:\n",
    "                    if not infected[int(neighbor)] and current_t in timestamps:\n",
    "                        if label_probabilities:\n",
    "                            infector_label = g.vp.label[node]\n",
    "                            infected_label = g.vp.label[neighbor]\n",
    "                            infection_prob = label_probabilities[(infector_label, infected_label)]\n",
    "                        else:\n",
    "                            infection_prob = 1\n",
    "\n",
    "                        if random() < infection_prob:\n",
    "                            infected[int(neighbor)] = True\n",
    "                            next_infected.append(neighbor)\n",
    "                            count += 1\n",
    "                            if count >= n:\n",
    "                                break\n",
    "\n",
    "            new_infected = next_infected\n",
    "            infected_count[current_t] += count\n",
    "\n",
    "    for node in g.vertices():\n",
    "        infect_node(node, t)\n",
    "\n",
    "    return [count / N for count in infected_count]\n",
    "\n",
    "# Example usage:\n",
    "# label_probabilities = {(\"A\", \"A\"): 0.9, (\"A\", \"B\"): 0.5, (\"B\", \"A\"): 0.5, (\"B\", \"B\"): 0.9}\n",
    "# infected_counts = SI_infection_simulation_timestamps(g, 10, 100, label_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing metrics\n",
    "def compute_metrics(G: gt.Graph, index_to_label: dict):\n",
    "\n",
    "    g = G\n",
    "\n",
    "    beta = 0.5\n",
    "    mu = 0.3\n",
    "\n",
    "    print(\"computing closeness\")\n",
    "    closeness = compute_closeness(g)\n",
    "\n",
    "    print(\"computing degree\")\n",
    "    degree = compute_degree(g)\n",
    "\n",
    "    print(\"computing betweenness\")\n",
    "    betweenness = compute_betweenness(g)\n",
    "\n",
    "    print(\"computing eigenvector_centrality\")\n",
    "    eigenvector_centrality = compute_eigenvector_centrality(g)\n",
    "\n",
    "    print(\"computing SIS_infection_prob\")\n",
    "    SIS_infection_prob = compute_SIS_infection_prob(g, beta, mu)\n",
    "\n",
    "    print(\"computing pagerank\")\n",
    "    page_rank = compute_page_rank(g)\n",
    "\n",
    "    print(\"computation done\")\n",
    "\n",
    "    # print(f\"Node : Betweenness : Eigenvector Centrality : Closeness : Degree : SIS Infection Probability : Page Rank\")\n",
    "    # for node in list(g.vertices())[:10]:\n",
    "    #     print(f\"{int(node)} : {betweenness[int(node)]} : {eigenvector_centrality[int(node)]} : {closeness[int(node)]} : {degree[int(node)]} : {SIS_infection_prob[int(node)]} : {page_rank[int(node)]}\")\n",
    "    #     # print(f\"Node {int(node)}: Betweenness: {betweenness[int(node)]}, Eigenvector Centrality: {eigenvector_centrality[int(node)]}, Closeness: {closeness[int(node)]}, Degree: {degree[int(node)]}, SIS Infection Probability: {SIS_infection_prob[int(node)]}\")\n",
    "    \n",
    "    data_dict = {}\n",
    "    for node in g.vertices():\n",
    "        data_dict[index_to_label[int(node)]] = dict(\n",
    "            closeness=closeness[int(node)],\n",
    "            degree=degree[int(node)],\n",
    "            betweenness=betweenness[int(node)],\n",
    "            eigenvector_centrality=eigenvector_centrality[int(node)],\n",
    "            sis_infection_prob=SIS_infection_prob[int(node)],\n",
    "            page_rank=page_rank[int(node)],\n",
    "        )\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "computing graph  only_users\n",
      "graph before filtering:  MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 789856/789856 [00:00<00:00, 2391846.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph after filtering: MultiDiGraph with 33732 nodes and 104702 edges\n",
      "part_1\n",
      "part_2\n",
      "part_3\n",
      "part_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104702/104702 [00:00<00:00, 311750.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing closeness\n",
      "computing degree\n",
      "computing betweenness\n",
      "computing eigenvector_centrality\n",
      "computing SIS_infection_prob\n",
      "computing pagerank\n",
      "computation done\n",
      "\n",
      "\n",
      "computing graph  users_and_cards\n",
      "graph before filtering:  MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 789856/789856 [00:00<00:00, 2483847.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph after filtering: MultiDiGraph with 152550 nodes and 232768 edges\n",
      "part_1\n",
      "part_2\n",
      "part_3\n",
      "part_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232768/232768 [00:00<00:00, 312711.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing closeness\n",
      "computing degree\n",
      "computing betweenness\n",
      "computing eigenvector_centrality\n",
      "computing SIS_infection_prob\n",
      "computing pagerank\n",
      "computation done\n",
      "\n",
      "\n",
      "computing graph  users_and_devices\n",
      "graph before filtering:  MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 789856/789856 [00:00<00:00, 2504828.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph after filtering: MultiDiGraph with 85183 nodes and 159728 edges\n",
      "part_1\n",
      "part_2\n",
      "part_3\n",
      "part_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159728/159728 [00:00<00:00, 315240.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing closeness\n",
      "computing degree\n",
      "computing betweenness\n",
      "computing eigenvector_centrality\n",
      "computing SIS_infection_prob\n",
      "computing pagerank\n",
      "computation done\n",
      "\n",
      "\n",
      "computing graph  users_and_ips\n",
      "graph before filtering:  MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 789856/789856 [00:00<00:00, 2609884.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph after filtering: MultiDiGraph with 619587 nodes and 1593651 edges\n",
      "part_1\n",
      "part_2\n",
      "part_3\n",
      "part_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1593651/1593651 [00:05<00:00, 307849.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing closeness\n",
      "computing degree\n",
      "computing betweenness\n",
      "computing eigenvector_centrality\n",
      "computing SIS_infection_prob\n",
      "computing pagerank\n",
      "computation done\n",
      "\n",
      "\n",
      "computing graph  users_and_cards_without_user_to_user_edges\n",
      "graph before filtering:  MultiDiGraph with 789856 nodes and 1776743 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 789856/789856 [00:00<00:00, 2589482.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph after filtering: MultiDiGraph with 152550 nodes and 0 edges\n",
      "part_1\n",
      "part_2\n",
      "part_3\n",
      "part_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing closeness\n",
      "computing degree\n",
      "computing betweenness\n",
      "computing eigenvector_centrality\n",
      "computing SIS_infection_prob\n",
      "computing pagerank\n",
      "computation done\n"
     ]
    }
   ],
   "source": [
    "# the experiment then goes like this:\n",
    "# 1. instantiate the graphs you're interested in\n",
    "# 2. run analysis on each, only keep the data_dict\n",
    "# 3. put the dicts together in a dict with simple names\n",
    "\n",
    "graphs_we_want = {\n",
    "    \"only_users\": dict(\n",
    "        node_labels_to_keep=['User'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_cards\": dict(\n",
    "        node_labels_to_keep=['User', 'Card'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_devices\": dict(\n",
    "        node_labels_to_keep=['User', 'Device'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_ips\": dict(\n",
    "        node_labels_to_keep=['User', 'IP'],\n",
    "        edges_to_keep=[],\n",
    "    ),\n",
    "    \"users_and_cards_without_user_to_user_edges\": dict(\n",
    "        node_labels_to_keep=['User', 'Card'],\n",
    "        edges_to_keep=[('User', 'Card')],\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for graph_name, graph_info in graphs_we_want.items():\n",
    "    print(\"\\n\\ncomputing graph \", graph_name)\n",
    "    # 1. instantiate the graphs you're interested in\n",
    "    nx_graph = filter_graph(nxG, graph_info['node_labels_to_keep'], graph_info['edges_to_keep'])\n",
    "    # 2. run analysis on each, only keep the data_dict\n",
    "    gt_graph, label_to_index, index_to_label = convert_networkx_to_graphtool(nx_graph)\n",
    "    result = compute_metrics(gt_graph, index_to_label)\n",
    "    # 3. put the dicts together in a dict with simple names\n",
    "    results[graph_name] = result\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"compute_metrics_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
